# ======== STOR566: Tiger run (Logistic + LeNet, CIFAR-10, 2 epochs) ========
# 0) Setup
!pip -q install timm
!git clone -q https://github.com/ziplab/LITv2.git

import os, time, random, math, json
from pathlib import Path
import numpy as np
import torch, torch.nn as nn, torch.nn.functional as F
import torchvision, torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler

import pandas as pd
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
import sys; sys.path.append('/content/LITv2')
from hilo import HiLo  # (HiLoViT will use this; not used in Tiger's block)

# 1) Reproducibility
def set_seed(seed=2025):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True
set_seed(2025)

# 2) Data
def get_cifar_loaders(dataset='cifar10', img_size=32, batch_size=128, num_workers=2):
    norm = T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))
    train_tf = T.Compose([T.RandomCrop(img_size, padding=4), T.RandomHorizontalFlip(), T.ToTensor(), norm])
    test_tf  = T.Compose([T.ToTensor(), norm])
    if dataset=='cifar100':
        train_ds = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_tf)
        ncls = 100
    else:
        train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_tf)
        ncls = 10
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return train_loader, test_loader, ncls

# 3) Models (Logistic, LeNet)
class LogisticRegressionNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.fc = nn.Linear(32*32*3, num_classes)
    def forward(self, x): return self.fc(x.view(x.size(0), -1))

class LeNetSmall(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 6, 5, padding=2), nn.ReLU(), nn.AvgPool2d(2),
            nn.Conv2d(6, 16, 5), nn.ReLU(), nn.AvgPool2d(2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(), nn.Linear(16*6*6, 120), nn.ReLU(),
            nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, num_classes)
        )
    def forward(self, x): return self.fc(self.conv(x))

# 4) Train / Eval
@torch.no_grad()
def eval_model(model, loader, criterion):
    model.eval()
    total, correct, loss_sum = 0, 0, 0.0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        with autocast(enabled=True):
            logits = model(x); loss = criterion(logits, y)
        pred = logits.argmax(1)
        correct += (pred==y).sum().item()
        total += y.size(0)
        loss_sum += loss.item()*y.size(0)
    return {'loss': loss_sum/total, 'top1': 100.0*correct/total}

def train_one(model, train_loader, test_loader, epochs=2, lr=1e-3, wd=0.0, name='model', outdir='./exp_outputs_Tiger'):
    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    scaler = GradScaler(enabled=True)
    crit = nn.CrossEntropyLoss().to(device)
    os.makedirs(outdir, exist_ok=True)
    hist = []
    for ep in range(1, epochs+1):
        model.train(); running=0.0; n=0; t0=time.time()
        for x,y in train_loader:
            x,y = x.to(device), y.to(device)
            opt.zero_grad(set_to_none=True)
            with autocast(enabled=True):
                logits = model(x); loss = crit(logits, y)
            scaler.scale(loss).backward()
            scaler.step(opt); scaler.update()
            running += loss.item()*y.size(0); n += y.size(0)
        sched.step()
        eval_res = eval_model(model, test_loader, crit)
        row = {'epoch':ep,'train_loss':running/n,'test_loss':eval_res['loss'],'test_top1':eval_res['top1'],'lr':sched.get_last_lr()[0]}
        hist.append(row)
        print(f"[{name}] Epoch {ep:02d}  top1={row['test_top1']:.2f}%  loss={row['test_loss']:.4f}")
    df = pd.DataFrame(hist)
    df.to_csv(os.path.join(outdir, f'history_cifar10_{name}.csv'), index=False)
    return df.iloc[-1]['test_top1']

def run_experiment(models=('logistic','lenet'), epochs=2, batch_size=128, outdir='./exp_outputs_Tiger'):
    train_loader, test_loader, ncls = get_cifar_loaders('cifar10', 32, batch_size)
    results = []
    for m in models:
        if m=='logistic':  net = LogisticRegressionNet(ncls); lr,wd=1e-3,0.0
        elif m=='lenet':   net = LeNetSmall(ncls); lr,wd=3e-4,1e-4
        else: raise ValueError(m)
        top1 = train_one(net, train_loader, test_loader, epochs=epochs, lr=lr, wd=wd, name=m, outdir=outdir)
        results.append({'model':m,'best_top1':top1})
    pd.DataFrame(results).to_csv(os.path.join(outdir,'summary_cifar10_Tiger.csv'), index=False)
    print("\nSaved:", os.path.join(outdir,'summary_cifar10_Tiger.csv'))

# 5) Go!
run_experiment(models=('logistic','lenet'), epochs=2, batch_size=128, outdir='./exp_outputs_Tiger')
