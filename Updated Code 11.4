# ===== STOR566 | Tiger run: Logistic + LeNet (CIFAR-10, 2 epochs) =====
# Setup
!pip -q install thop
import os, time, random, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
import torchvision, torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from thop import profile
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Reproducibility
def set_seed(s=2025):
    random.seed(s); np.random.seed(s)
    torch.manual_seed(s); torch.cuda.manual_seed_all(s)
    torch.backends.cudnn.benchmark = True
set_seed(2025)

# Data
def get_cifar_loaders(dataset='cifar10', img_size=32, batch_size=128, num_workers=2):
    norm = T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))
    train_tf = T.Compose([T.RandomCrop(img_size, padding=4), T.RandomHorizontalFlip(), T.ToTensor(), norm])
    test_tf  = T.Compose([T.ToTensor(), norm])
    if dataset=='cifar100':
        train_ds = torchvision.datasets.CIFAR100('./data', train=True,  download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)
        ncls = 100
    else:
        train_ds = torchvision.datasets.CIFAR10('./data', train=True,  download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=test_tf)
        ncls = 10
    tr = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)
    te = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return tr, te, ncls

# Models
class LogisticRegressionNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.fc = nn.Linear(32*32*3, num_classes)
    def forward(self, x):
        return self.fc(x.view(x.size(0), -1))

class LeNetSmall(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3,6,5,padding=2), nn.ReLU(), nn.AvgPool2d(2),
            nn.Conv2d(6,16,5), nn.ReLU(), nn.AvgPool2d(2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(), nn.Linear(16*6*6,120), nn.ReLU(),
            nn.Linear(120,84), nn.ReLU(), nn.Linear(84,num_classes)
        )
    def forward(self, x): return self.fc(self.conv(x))

# Metrics helpers
def count_params(m): return sum(p.numel() for p in m.parameters())
def get_flops(model, input_size=(1,3,32,32)):
    m = model.to('cpu').eval()
    x = torch.randn(*input_size)
    try:
        flops, params = profile(m, inputs=(x,), verbose=False)
    except Exception:
        flops, params = -1, -1
    return int(flops), int(params)

@torch.no_grad()
def eval_with_timing(model, loader, criterion):
    model.eval(); total=0; correct=0; loss_sum=0.0; times=[]
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        if device.type=='cuda': torch.cuda.synchronize()
        t0=time.time()
        with autocast(True):
            logits=model(x); loss=criterion(logits,y)
        if device.type=='cuda': torch.cuda.synchronize()
        times.append(time.time()-t0)
        pred=logits.argmax(1); correct+=(pred==y).sum().item()
        total+=y.size(0); loss_sum+=loss.item()*y.size(0)
    avg_batch = float(np.mean(times)) if times else float('nan')
    thr = loader.batch_size/avg_batch if avg_batch>0 else float('nan')
    return {'loss':loss_sum/total,'top1':100.0*correct/total,'avg_infer_batch_s':avg_batch,'throughput_img_s':thr}

def train_with_metrics(model, train_loader, test_loader, *, epochs, lr, wd, name, outdir):
    os.makedirs(outdir, exist_ok=True)
    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    scaler = GradScaler(True); crit = nn.CrossEntropyLoss().to(device)
    flops,_ = get_flops(model); params = count_params(model)
    hist=[]; t0=time.time()
    if device.type=='cuda': torch.cuda.reset_peak_memory_stats()
    for ep in range(1,epochs+1):
        model.train(); run=0.0; n=0; ep0=time.time()
        for x,y in train_loader:
            x,y=x.to(device),y.to(device)
            opt.zero_grad(set_to_none=True)
            with autocast(True):
                logits=model(x); loss=crit(logits,y)
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
            run += loss.item()*y.size(0); n += y.size(0)
        sch.step()
        eval_res = eval_with_timing(model, test_loader, crit)
        ep_time = time.time()-ep0
        peak = (torch.cuda.max_memory_allocated()/1024**2) if device.type=='cuda' else float('nan')
        row={'epoch':ep,'train_loss':run/n,'test_loss':eval_res['loss'],'test_top1':eval_res['top1'],
             'avg_infer_batch_s':eval_res['avg_infer_batch_s'],'throughput_img_s':eval_res['throughput_img_s'],
             'epoch_time_s':ep_time,'peak_gpu_mem_MB':peak,'lr':sch.get_last_lr()[0]}
        hist.append(row)
        print(f"[{name}] ep{ep:02d} top1={row['test_top1']:.2f}% loss={row['test_loss']:.4f} "
              f"time/ep={ep_time:.1f}s thr={row['throughput_img_s']:.1f}/s")
    total_train = time.time()-t0
    pd.DataFrame(hist).to_csv(os.path.join(outdir,f'history_cifar10_{name}.csv'), index=False)
    final = hist[-1]
    return {'model':name,'best_top1':max(h['test_top1'] for h in hist),'params':params,'FLOPs_thop':flops,
            'avg_infer_batch_s':final['avg_infer_batch_s'],'throughput_img_s':final['throughput_img_s'],
            'total_train_time_s':total_train,'peak_gpu_mem_MB':final['peak_gpu_mem_MB']}

# Run
train_loader, test_loader, ncls = get_cifar_loaders('cifar10', 32, 128)
summ = []
summ.append(train_with_metrics(LogisticRegressionNet(ncls), train_loader, test_loader,
                               epochs=2, lr=1e-3, wd=0.0, name='logistic', outdir='./exp_outputs_Tiger'))
summ.append(train_with_metrics(LeNetSmall(ncls), train_loader, test_loader,
                               epochs=2, lr=3e-4, wd=1e-4, name='lenet', outdir='./exp_outputs_Tiger'))
pd.DataFrame(summ).to_csv('./exp_outputs_Tiger/summary_cifar10_Tiger.csv', index=False)
print("Saved -> ./exp_outputs_Tiger/summary_cifar10_Tiger.csv")


# ===== STOR566 | Lizzy run: AlexNet + Traditional Transformer (CIFAR-10, 2 epochs) =====
# Setup
!pip -q install thop
import os, time, random, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
import torchvision, torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from thop import profile
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Seed
def set_seed(s=2025):
    random.seed(s); np.random.seed(s)
    torch.manual_seed(s); torch.cuda.manual_seed_all(s)
    torch.backends.cudnn.benchmark = True
set_seed(2025)

# Data
def get_cifar_loaders(dataset='cifar10', img_size=32, batch_size=128, num_workers=2):
    norm = T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))
    train_tf = T.Compose([T.RandomCrop(img_size, padding=4), T.RandomHorizontalFlip(), T.ToTensor(), norm])
    test_tf  = T.Compose([T.ToTensor(), norm])
    train_ds = torchvision.datasets.CIFAR10('./data', train=True,  download=True, transform=train_tf)
    test_ds  = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=test_tf)
    tr = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)
    te = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return tr, te, 10

# AlexNet (32x32)
class AlexNet32(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3,64,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64,192,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(192,384,3,1,1), nn.ReLU(),
            nn.Conv2d(384,256,3,1,1), nn.ReLU(),
            nn.Conv2d(256,256,3,1,1), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5), nn.Linear(256*4*4, 4096), nn.ReLU(),
            nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(),
            nn.Linear(4096, num_classes)
        )
    def forward(self, x):
        x=self.features(x); x=torch.flatten(x,1); return self.classifier(x)

# ===== Traditional Transformer (ViT-MSA) =====
class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_ch=3, embed_dim=192):
        super().__init__()
        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.grid = img_size // patch_size
    def forward(self, x):
        x = self.proj(x)                 # B, C, H', W'
        x = x.flatten(2).transpose(1,2)  # B, N, C
        return x

class ViTMSA(nn.Module):
    """Minimal ViT with standard Multi-Head Self-Attention (no HiLo)."""
    def __init__(self, img_size=32, patch_size=4, num_classes=10,
                 embed_dim=192, depth=6, num_heads=6, mlp_ratio=4.0, drop=0.0):
        super().__init__()
        self.patch = PatchEmbed(img_size, patch_size, 3, embed_dim)
        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))
        self.pos = nn.Parameter(torch.zeros(1, (img_size//patch_size)**2 + 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim*mlp_ratio),
            dropout=drop, activation='gelu', batch_first=True, norm_first=True
        )
        self.blocks = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
        nn.init.trunc_normal_(self.pos, std=0.02); nn.init.trunc_normal_(self.cls_token, std=0.02)
        self.grid = img_size//patch_size
    def forward(self, x):
        B = x.size(0)
        x = self.patch(x)                        # B, N, C
        cls = self.cls_token.expand(B, -1, -1)   # B,1,C
        x = torch.cat([cls,x], dim=1) + self.pos # add pos
        x = self.blocks(x)                       # standard MSA
        x = self.norm(x)
        cls = x[:,0]
        return self.head(cls)

# Metrics helpers
def count_params(m): return sum(p.numel() for p in m.parameters())
def get_flops(model, input_size=(1,3,32,32)):
    m = model.to('cpu').eval()
    x = torch.randn(*input_size)
    try:
        flops, params = profile(m, inputs=(x,), verbose=False)
    except Exception:
        flops, params = -1, -1
    return int(flops), int(params)

@torch.no_grad()
def eval_with_timing(model, loader, criterion):
    model.eval(); total=0; correct=0; loss_sum=0.0; times=[]
    for x,y in loader:
        x,y=x.to(device),y.to(device)
        if device.type=='cuda': torch.cuda.synchronize()
        t0=time.time()
        with autocast(True):
            logits=model(x); loss=criterion(logits,y)
        if device.type=='cuda': torch.cuda.synchronize()
        times.append(time.time()-t0)
        pred=logits.argmax(1); correct+=(pred==y).sum().item()
        total+=y.size(0); loss_sum+=loss.item()*y.size(0)
    avg_batch=float(np.mean(times)) if times else float('nan')
    thr=loader.batch_size/avg_batch if avg_batch>0 else float('nan')
    return {'loss':loss_sum/total,'top1':100.0*correct/total,'avg_infer_batch_s':avg_batch,'throughput_img_s':thr}

def train_with_metrics(model, train_loader, test_loader, *, epochs, lr, wd, name, outdir):
    os.makedirs(outdir, exist_ok=True)
    model=model.to(device)
    opt=torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sch=torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    scaler=GradScaler(True); crit=nn.CrossEntropyLoss().to(device)
    flops,_=get_flops(model); params=count_params(model)
    hist=[]; t0=time.time()
    if device.type=='cuda': torch.cuda.reset_peak_memory_stats()
    for ep in range(1,epochs+1):
        model.train(); run=0.0; n=0; ep0=time.time()
        for x,y in train_loader:
            x,y=x.to(device),y.to(device)
            opt.zero_grad(set_to_none=True)
            with autocast(True):
                logits=model(x); loss=crit(logits,y)
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
            run+=loss.item()*y.size(0); n+=y.size(0)
        sch.step()
        ev=eval_with_timing(model,test_loader,crit)
        ep_time=time.time()-ep0
        peak=(torch.cuda.max_memory_allocated()/1024**2) if device.type=='cuda' else float('nan')
        row={'epoch':ep,'train_loss':run/n,'test_loss':ev['loss'],'test_top1':ev['top1'],
             'avg_infer_batch_s':ev['avg_infer_batch_s'],'throughput_img_s':ev['throughput_img_s'],
             'epoch_time_s':ep_time,'peak_gpu_mem_MB':peak,'lr':sch.get_last_lr()[0]}
        hist.append(row)
        print(f"[{name}] ep{ep:02d} top1={row['test_top1']:.2f}% loss={row['test_loss']:.4f} time/ep={ep_time:.1f}s")
    total_train=time.time()-t0
    pd.DataFrame(hist).to_csv(os.path.join(outdir,f'history_cifar10_{name}.csv'), index=False)
    final=hist[-1]
    return {'model':name,'best_top1':max(h['test_top1'] for h in hist),'params':params,'FLOPs_thop':flops,
            'avg_infer_batch_s':final['avg_infer_batch_s'],'throughput_img_s':final['throughput_img_s'],
            'total_train_time_s':total_train,'peak_gpu_mem_MB':final['peak_gpu_mem_MB']}

# Run
train_loader, test_loader, ncls = get_cifar_loaders('cifar10', 32, 128)
summ=[]
summ.append(train_with_metrics(AlexNet32(ncls), train_loader, test_loader,
                               epochs=2, lr=3e-4, wd=1e-4, name='alexnet', outdir='./exp_outputs_Lizzy'))
summ.append(train_with_metrics(ViTMSA(img_size=32, patch_size=4, num_classes=ncls,
                                      embed_dim=192, depth=6, num_heads=6, mlp_ratio=4.0),
                               train_loader, test_loader, epochs=2, lr=3e-4, wd=1e-4,
                               name='vit_msa', outdir='./exp_outputs_Lizzy'))
pd.DataFrame(summ).to_csv('./exp_outputs_Lizzy/summary_cifar10_Lizzy.csv', index=False)
print("Saved -> ./exp_outputs_Lizzy/summary_cifar10_Lizzy.csv")

# ===== STOR566 | Shawn run: ResNet18 + HiLoViT (CIFAR-10, 2 epochs) =====
# Setup
!pip -q install thop timm
!git clone -q https://github.com/ziplab/LITv2.git
import sys; sys.path.append('/content/LITv2')
from hilo import HiLo  # official HiLo attention

import os, time, random, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
import torchvision, torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from thop import profile
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Seed
def set_seed(s=2025):
    random.seed(s); np.random.seed(s)
    torch.manual_seed(s); torch.cuda.manual_seed_all(s)
    torch.backends.cudnn.benchmark = True
set_seed(2025)

# Data
def get_cifar_loaders(dataset='cifar10', img_size=32, batch_size=128, num_workers=2):
    norm = T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))
    train_tf = T.Compose([T.RandomCrop(img_size, padding=4), T.RandomHorizontalFlip(), T.ToTensor(), norm])
    test_tf  = T.Compose([T.ToTensor(), norm])
    train_ds = torchvision.datasets.CIFAR10('./data', train=True,  download=True, transform=train_tf)
    test_ds  = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=test_tf)
    tr = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)
    te = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return tr, te, 10

# ResNet18
def get_resnet18(num_classes=10):
    m = torchvision.models.resnet18(weights=None)
    m.fc = nn.Linear(m.fc.in_features, num_classes)
    return m

# Minimal ViT with HiLo blocks (using LITv2 HiLo)
class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_ch=3, embed_dim=192):
        super().__init__()
        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.grid = img_size // patch_size
    def forward(self, x):
        x = self.proj(x); x = x.flatten(2).transpose(1,2); return x  # B,N,C

class MLP(nn.Module):
    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):
        super().__init__()
        h=int(dim*mlp_ratio)
        self.fc1=nn.Linear(dim,h); self.fc2=nn.Linear(h,dim)
        self.act=nn.GELU(); self.drop=nn.Dropout(drop)
    def forward(self, x):
        x=self.fc1(x); x=self.act(x); x=self.drop(x)
        x=self.fc2(x); x=self.drop(x); return x

class HiLoBlock(nn.Module):
    def __init__(self, dim, num_heads=6, window_size=2, alpha=0.5, drop=0.0, mlp_ratio=4.0):
        super().__init__()
        self.norm1=nn.LayerNorm(dim); self.attn=HiLo(dim, num_heads, window_size, alpha)
        self.norm2=nn.LayerNorm(dim); self.mlp=MLP(dim, mlp_ratio, drop)
        self.drop_path=nn.Identity()
    def forward(self, x, H, W):
        x = x + self.drop_path(self.attn(self.norm1(x), H, W))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class HiLoViT(nn.Module):
    def __init__(self, img_size=32, patch_size=4, num_classes=10,
                 embed_dim=192, depth=6, num_heads=6, window_size=2, alpha=0.5):
        super().__init__()
        self.patch=PatchEmbed(img_size, patch_size, 3, embed_dim)
        self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))
        self.pos=nn.Parameter(torch.zeros(1,(img_size//patch_size)**2+1,embed_dim))
        self.blocks=nn.ModuleList([HiLoBlock(embed_dim, num_heads, window_size, alpha) for _ in range(depth)])
        self.norm=nn.LayerNorm(embed_dim); self.head=nn.Linear(embed_dim, num_classes)
        self.grid=img_size//patch_size
        nn.init.trunc_normal_(self.pos, std=0.02); nn.init.trunc_normal_(self.cls_token, std=0.02)
    def forward(self, x):
        B=x.size(0); x=self.patch(x); cls=self.cls_token.expand(B,-1,-1)
        x=torch.cat([cls,x],1)+self.pos
        for blk in self.blocks: x=blk(x, self.grid, self.grid)
        x=self.norm(x); cls=x[:,0]; return self.head(cls)

# Metrics helpers
def count_params(m): return sum(p.numel() for p in m.parameters())
def get_flops(model, input_size=(1,3,32,32)):
    m = model.to('cpu').eval()
    x = torch.randn(*input_size)
    try:
        flops, params = profile(m, inputs=(x,), verbose=False)
    except Exception:
        flops, params = -1, -1
    return int(flops), int(params)

@torch.no_grad()
def eval_with_timing(model, loader, criterion):
    model.eval(); total=0; correct=0; loss_sum=0.0; times=[]
    for x,y in loader:
        x,y=x.to(device),y.to(device)
        if device.type=='cuda': torch.cuda.synchronize()
        t0=time.time()
        with autocast(True):
            logits=model(x); loss=criterion(logits,y)
        if device.type=='cuda': torch.cuda.synchronize()
        times.append(time.time()-t0)
        pred=logits.argmax(1); correct+=(pred==y).sum().item()
        total+=y.size(0); loss_sum+=loss.item()*y.size(0)
    avg_batch=float(np.mean(times)) if times else float('nan')
    thr=loader.batch_size/avg_batch if avg_batch>0 else float('nan')
    return {'loss':loss_sum/total,'top1':100.0*correct/total,'avg_infer_batch_s':avg_batch,'throughput_img_s':thr}

def train_with_metrics(model, train_loader, test_loader, *, epochs, lr, wd, name, outdir):
    os.makedirs(outdir, exist_ok=True)
    model=model.to(device)
    opt=torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sch=torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    scaler=GradScaler(True); crit=nn.CrossEntropyLoss().to(device)
    flops,_=get_flops(model); params=count_params(model)
    hist=[]; t0=time.time()
    if device.type=='cuda': torch.cuda.reset_peak_memory_stats()
    for ep in range(1,epochs+1):
        model.train(); run=0.0; n=0; ep0=time.time()
        for x,y in train_loader:
            x,y=x.to(device),y.to(device)
            opt.zero_grad(set_to_none=True)
            with autocast(True):
                logits=model(x); loss=crit(logits,y)
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
            run+=loss.item()*y.size(0); n+=y.size(0)
        sch.step()
        ev=eval_with_timing(model,test_loader,crit)
        ep_time=time.time()-ep0
        peak=(torch.cuda.max_memory_allocated()/1024**2) if device.type=='cuda' else float('nan')
        row={'epoch':ep,'train_loss':run/n,'test_loss':ev['loss'],'test_top1':ev['top1'],
             'avg_infer_batch_s':ev['avg_infer_batch_s'],'throughput_img_s':ev['throughput_img_s'],
             'epoch_time_s':ep_time,'peak_gpu_mem_MB':peak,'lr':sch.get_last_lr()[0]}
        hist.append(row)
        print(f"[{name}] ep{ep:02d} top1={row['test_top1']:.2f}% loss={row['test_loss']:.4f} thr={row['throughput_img_s']:.1f}/s")
    total_train=time.time()-t0
    pd.DataFrame(hist).to_csv(os.path.join(outdir,f'history_cifar10_{name}.csv'), index=False)
    final=hist[-1]
    return {'model':name,'best_top1':max(h['test_top1'] for h in hist),'params':params,'FLOPs_thop':flops,
            'avg_infer_batch_s':final['avg_infer_batch_s'],'throughput_img_s':final['throughput_img_s'],
            'total_train_time_s':total_train,'peak_gpu_mem_MB':final['peak_gpu_mem_MB']}

# Run
train_loader, test_loader, ncls = get_cifar_loaders('cifar10', 32, 128)
summ=[]
summ.append(train_with_metrics(get_resnet18(ncls), train_loader, test_loader,
                               epochs=2, lr=3e-4, wd=1e-4, name='resnet18', outdir='./exp_outputs_Shawn'))
summ.append(train_with_metrics(HiLoViT(img_size=32, num_classes=ncls, embed_dim=192, depth=6, num_heads=6, window_size=2, alpha=0.5),
                               train_loader, test_loader, epochs=2, lr=3e-4, wd=1e-4,
                               name='hilovit', outdir='./exp_outputs_Shawn'))
pd.DataFrame(summ).to_csv('./exp_outputs_Shawn/summary_cifar10_Shawn.csv', index=False)
print("Saved -> ./exp_outputs_Shawn/summary_cifar10_Shawn.csv")
