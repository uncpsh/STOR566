# ===== STOR566 | Tiger run: Logistic + LeNet (CIFAR-10, 2 epochs) =====
# Setup
!pip -q install thop
import os, time, random, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
import torchvision, torchvision.transforms as T
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from thop import profile
from google.colab import files

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Reproducibility
def set_seed(s=2025):
    random.seed(s); np.random.seed(s)
    torch.manual_seed(s); torch.cuda.manual_seed_all(s)
    torch.backends.cudnn.benchmark = True
set_seed(2025)

# Data
def get_cifar_loaders(dataset='cifar10', img_size=32, batch_size=128, num_workers=2):
    norm = T.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))
    train_tf = T.Compose([T.RandomCrop(img_size, padding=4), T.RandomHorizontalFlip(), T.ToTensor(), norm])
    test_tf  = T.Compose([T.ToTensor(), norm])
    if dataset=='cifar100':
        train_ds = torchvision.datasets.CIFAR100('./data', train=True,  download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)
        ncls = 100
    else:
        train_ds = torchvision.datasets.CIFAR10('./data', train=True,  download=True, transform=train_tf)
        test_ds  = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=test_tf)
        ncls = 10
    tr = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)
    te = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return tr, te, ncls

# Models
class LogisticRegressionNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.fc = nn.Linear(32*32*3, num_classes)
    def forward(self, x):
        return self.fc(x.view(x.size(0), -1))

class LeNetSmall(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3,6,5,padding=2), nn.ReLU(), nn.AvgPool2d(2),
            nn.Conv2d(6,16,5), nn.ReLU(), nn.AvgPool2d(2)
        )
        self.fc = nn.Sequential(
            nn.Flatten(), nn.Linear(16*6*6,120), nn.ReLU(),
            nn.Linear(120,84), nn.ReLU(), nn.Linear(84,num_classes)
        )
    def forward(self, x): 
        return self.fc(self.conv(x))

# Metrics helpers
def count_params(m): 
    return sum(p.numel() for p in m.parameters())

def get_flops(model, input_size=(1,3,32,32)):
    m = model.to('cpu').eval()
    x = torch.randn(*input_size)
    try:
        flops, params = profile(m, inputs=(x,), verbose=False)
    except Exception:
        flops, params = -1, -1
    return int(flops), int(params)

@torch.no_grad()
def eval_with_timing(model, loader, criterion):
    model.eval(); total=0; correct=0; loss_sum=0.0; times=[]
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        if device.type=='cuda': torch.cuda.synchronize()
        t0=time.time()
        with autocast(enabled=(device.type=='cuda')):
            logits=model(x); loss=criterion(logits,y)
        if device.type=='cuda': torch.cuda.synchronize()
        times.append(time.time()-t0)
        pred=logits.argmax(1); correct+=(pred==y).sum().item()
        total+=y.size(0); loss_sum+=loss.item()*y.size(0)
    avg_batch = float(np.mean(times)) if times else float('nan')
    thr = loader.batch_size/avg_batch if avg_batch>0 else float('nan')
    return {'loss':loss_sum/total,
            'top1':100.0*correct/total,
            'avg_infer_batch_s':avg_batch,
            'throughput_img_s':thr}

def train_with_metrics(model, train_loader, test_loader, *, epochs, lr, wd, name, outdir):
    os.makedirs(outdir, exist_ok=True)
    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)
    scaler = GradScaler(enabled=(device.type=='cuda'))
    crit = nn.CrossEntropyLoss().to(device)

    flops,_ = get_flops(model)
    model.to(device)  # move back after FLOPs
    params = count_params(model)

    hist=[]; t0=time.time()
    if device.type=='cuda': torch.cuda.reset_peak_memory_stats()

    num_train_imgs = len(train_loader.dataset)

    for ep in range(1,epochs+1):
        model.train(); run=0.0; n=0; ep0=time.time()
        for x,y in train_loader:
            x,y=x.to(device),y.to(device)
            opt.zero_grad(set_to_none=True)
            with autocast(enabled=(device.type=='cuda')):
                logits=model(x); loss=crit(logits,y)
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
            run += loss.item()*y.size(0); n += y.size(0)
        sch.step()
        eval_res = eval_with_timing(model, test_loader, crit)
        ep_time = time.time()-ep0
        peak = (torch.cuda.max_memory_allocated()/1024**2) if device.type=='cuda' else float('nan')
        row={'epoch':ep,
             'train_loss':run/n,
             'test_loss':eval_res['loss'],
             'test_top1':eval_res['top1'],
             'avg_infer_batch_s':eval_res['avg_infer_batch_s'],
             'throughput_img_s':eval_res['throughput_img_s'],
             'epoch_time_s':ep_time,
             'peak_gpu_mem_MB':peak,
             'lr':sch.get_last_lr()[0]}
        hist.append(row)
        print(f"[{name}] ep{ep:02d} top1={row['test_top1']:.2f}% "
              f"loss={row['test_loss']:.4f} time/ep={ep_time:.1f}s "
              f"thr={row['throughput_img_s']:.1f}/s")

    total_train = time.time()-t0
    hist_df = pd.DataFrame(hist)
    hist_df.to_csv(os.path.join(outdir,f'history_cifar10_{name}.csv'), index=False)

    # ----- aggregate metrics for CSV summary -----
    tops = hist_df['test_top1'].tolist()
    train_losses = hist_df['train_loss'].tolist()
    test_losses = hist_df['test_loss'].tolist()
    epoch_times = hist_df['epoch_time_s'].tolist()
    cum_times = np.cumsum(epoch_times)

    best_top1 = max(tops)
    best_idx = int(np.argmax(tops))
    epoch_of_best = int(hist_df.iloc[best_idx]['epoch'])

    # time to 50% and 90% of best
    t50 = float('nan'); t90 = float('nan')
    for i,acc in enumerate(tops):
        if np.isnan(t50) and acc >= 0.5*best_top1:
            t50 = float(cum_times[i])
        if np.isnan(t90) and acc >= 0.9*best_top1:
            t90 = float(cum_times[i])
    avg_epoch_time = float(np.mean(epoch_times))
    train_throughput = num_train_imgs / avg_epoch_time if avg_epoch_time>0 else float('nan')
    peak_gpu_mem = float(np.nanmax(hist_df['peak_gpu_mem_MB'].values))

    summary = {
        'model': name,
        'best_top1': best_top1,
        'epoch_of_best_top1': epoch_of_best,
        'train_loss_first_epoch': train_losses[0],
        'train_loss_last_epoch': train_losses[-1],
        'test_loss_first_epoch': test_losses[0],
        'test_loss_last_epoch': test_losses[-1],
        'std_train_loss': float(np.std(train_losses)),
        'std_test_top1': float(np.std(tops)),
        'params': params,
        'FLOPs_thop': flops,
        'avg_epoch_time_s': avg_epoch_time,
        'total_train_time_s': total_train,
        'time_to_50pct_of_best_s': t50,
        'time_to_90pct_of_best_s': t90,
        'train_throughput_img_s': train_throughput,
        'avg_infer_batch_s': float(hist_df.iloc[-1]['avg_infer_batch_s']),
        'infer_throughput_img_s': float(hist_df.iloc[-1]['throughput_img_s']),
        'peak_gpu_mem_MB': peak_gpu_mem,
        'final_lr': float(hist_df.iloc[-1]['lr'])
    }
    return summary

# Run
train_loader, test_loader, ncls = get_cifar_loaders('cifar10', 32, 128)
outdir = './exp_outputs_Tiger'
os.makedirs(outdir, exist_ok=True)

summ = []
summ.append(train_with_metrics(
    LogisticRegressionNet(ncls), train_loader, test_loader,
    epochs=2, lr=1e-3, wd=0.0, name='logistic', outdir=outdir))

summ.append(train_with_metrics(
    LeNetSmall(ncls), train_loader, test_loader,
    epochs=2, lr=3e-4, wd=1e-4, name='lenet', outdir=outdir))

summary_path = os.path.join(outdir, 'summary_cifar10_Tiger.csv')
pd.DataFrame(summ).to_csv(summary_path, index=False)
print("Saved ->", summary_path)

# download to local machine
files.download(summary_path)
